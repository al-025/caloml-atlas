{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#import libraries and some constants\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize, LogNorm\n",
    "import pandas as pd\n",
    "import uproot as ur\n",
    "import pickle\n",
    "import atlas_mpl_style as ampl\n",
    "ampl.use_atlas_style()\n",
    "\n",
    "path_prefix = '/AL/Phd/maxml/caloml-atlas/'\n",
    "plotpath = path_prefix+'classifier/Plots/'\n",
    "modelpath = path_prefix+'classifier/Models/'\n",
    "\n",
    "# import our resolution utilities\n",
    "import sys\n",
    "sys.path.append(path_prefix)\n",
    "from  util import resolution_util as ru\n",
    "from  util import plot_util as pu\n",
    "from  util import ml_util as mu\n",
    "\n",
    "inputpath = path_prefix+'inputs/'\n",
    "rootfiles = [\"pi0\", \"piplus\", \"piminus\"]\n",
    "\n",
    "trees, pdata = mu.setupPionData(inputpath, rootfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pi0 events: 263891\n",
      "Number of pi+ events: 435967\n",
      "Number of pi- events: 434627\n",
      "Total: 1134485\n"
     ]
    }
   ],
   "source": [
    "np0 = len(pdata['pi0'])\n",
    "npp = len(pdata['piplus'])\n",
    "npm = len(pdata['piminus'])\n",
    "\n",
    "print(\"Number of pi0 events: {}\".format(np0))\n",
    "print(\"Number of pi+ events: {}\".format(npp))\n",
    "print(\"Number of pi- events: {}\".format(npm))\n",
    "print(\"Total: {}\".format(np0+npp+npm))\n",
    "\n",
    "pcells = {\n",
    "    ifile : {\n",
    "        layer : mu.setupCells(itree, layer, flatten = False)\n",
    "        for layer in mu.cell_meta\n",
    "    }\n",
    "    for ifile, itree in trees.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "Number of devices: 1\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Convolution2D\n",
    "from tensorflow.keras.layers import MaxPool2D\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow import keras as keras\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "gpu_list = [\"/gpu:0\"]\n",
    "strategy = tf.distribute.MirroredStrategy(devices=gpu_list)\n",
    "# strategy = tf.distribute.MirroredStrategy()\n",
    "ngpu = strategy.num_replicas_in_sync\n",
    "print ('Number of devices: {}'.format(ngpu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_classes = ['pi0','piplus']\n",
    "# create train/validation/test subsets containing 70%/10%/20%\n",
    "# of events from each type of pion event\n",
    "pdata_merged, pcells_merged, plabels = mu.createTrainingDatasets(training_classes, pdata, pcells)\n",
    "\n",
    "pcells_merged_reshaped = mu.reshapeSeparateCNN(pcells_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = {\n",
    "    'EMB1': (1,1), \n",
    "    'EMB2': (1,1), \n",
    "    'EMB3': (1,1), \n",
    "    'TileBar0': (1,1), \n",
    "    'TileBar1': (1,1), \n",
    "    'TileBar2': (1,1)\n",
    "}\n",
    "filters2 = {\n",
    "    'EMB1': (4,8), \n",
    "    'EMB2': (8,8), \n",
    "    'EMB3': (4,2), \n",
    "    'TileBar0': (2,2), \n",
    "    'TileBar1': (2,2), \n",
    "    'TileBar2': (1,1)\n",
    "}\n",
    "pools2 = {\n",
    "    'EMB1': (2,2), \n",
    "    'EMB2': (2,2), \n",
    "    'EMB3': (1,1), \n",
    "    'TileBar0': (1,1), \n",
    "    'TileBar1': (1,1), \n",
    "    'TileBar2': (1,1)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model_layers(layer):\n",
    "    print(layer)\n",
    "    # create model\n",
    "    with strategy.scope():\n",
    "        model = Sequential()\n",
    "        model.add(Convolution2D(32, filters[layer], input_shape=(1,mu.cell_meta[layer]['len_eta'],mu.cell_meta[layer]['len_phi']), activation='relu', data_format = 'channels_first'))\n",
    "        model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "        model.add(Convolution2D(16, pools2[layer], activation='relu'))\n",
    "        model.add(MaxPool2D(pool_size=pools2[layer]))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(128, activation='relu'))\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dense(2, kernel_initializer='normal', activation='softmax'))\n",
    "        # compile model\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMB1\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "EMB2\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "EMB3\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "TileBar0\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "TileBar1\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "TileBar2\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    layer: cnn_model_layers(layer)\n",
    "    for layer in mu.cell_meta\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 489899 samples, validate on 69988 samples\n",
      "Epoch 1/100\n",
      "489899/489899 - 31s - loss: 0.4050 - acc: 0.8187 - val_loss: 0.3827 - val_acc: 0.8354\n",
      "Epoch 2/100\n",
      "489899/489899 - 23s - loss: 0.3403 - acc: 0.8603 - val_loss: 0.2971 - val_acc: 0.8847\n",
      "Epoch 3/100\n",
      "489899/489899 - 23s - loss: 0.2909 - acc: 0.8861 - val_loss: 0.2845 - val_acc: 0.8901\n",
      "Epoch 4/100\n",
      "489899/489899 - 23s - loss: 0.2779 - acc: 0.8913 - val_loss: 0.2708 - val_acc: 0.8966\n",
      "Epoch 5/100\n",
      "489899/489899 - 23s - loss: 0.2713 - acc: 0.8940 - val_loss: 0.2666 - val_acc: 0.8965\n",
      "Epoch 6/100\n",
      "489899/489899 - 23s - loss: 0.2661 - acc: 0.8964 - val_loss: 0.2679 - val_acc: 0.8976\n",
      "Epoch 7/100\n",
      "489899/489899 - 23s - loss: 0.2612 - acc: 0.8984 - val_loss: 0.2629 - val_acc: 0.9004\n",
      "Epoch 8/100\n",
      "489899/489899 - 23s - loss: 0.2579 - acc: 0.8998 - val_loss: 0.2583 - val_acc: 0.9006\n",
      "Epoch 9/100\n",
      "489899/489899 - 23s - loss: 0.2563 - acc: 0.9005 - val_loss: 0.2599 - val_acc: 0.9005\n",
      "Epoch 10/100\n",
      "489899/489899 - 23s - loss: 0.2544 - acc: 0.9010 - val_loss: 0.2511 - val_acc: 0.9034\n",
      "Epoch 11/100\n",
      "489899/489899 - 23s - loss: 0.2529 - acc: 0.9015 - val_loss: 0.2546 - val_acc: 0.9013\n",
      "Epoch 12/100\n",
      "489899/489899 - 23s - loss: 0.2502 - acc: 0.9028 - val_loss: 0.2511 - val_acc: 0.9030\n",
      "Epoch 13/100\n",
      "489899/489899 - 23s - loss: 0.2482 - acc: 0.9033 - val_loss: 0.2485 - val_acc: 0.9038\n",
      "Epoch 14/100\n",
      "489899/489899 - 23s - loss: 0.2465 - acc: 0.9042 - val_loss: 0.2508 - val_acc: 0.9041\n",
      "Epoch 15/100\n",
      "489899/489899 - 23s - loss: 0.2459 - acc: 0.9041 - val_loss: 0.2452 - val_acc: 0.9052\n",
      "Epoch 16/100\n",
      "489899/489899 - 23s - loss: 0.2439 - acc: 0.9051 - val_loss: 0.2455 - val_acc: 0.9042\n",
      "Epoch 17/100\n",
      "489899/489899 - 23s - loss: 0.2436 - acc: 0.9052 - val_loss: 0.2432 - val_acc: 0.9056\n",
      "Epoch 18/100\n",
      "489899/489899 - 22s - loss: 0.2432 - acc: 0.9051 - val_loss: 0.2497 - val_acc: 0.9030\n",
      "Epoch 19/100\n",
      "489899/489899 - 22s - loss: 0.2413 - acc: 0.9063 - val_loss: 0.2554 - val_acc: 0.9011\n",
      "Epoch 20/100\n",
      "489899/489899 - 22s - loss: 0.2399 - acc: 0.9067 - val_loss: 0.2423 - val_acc: 0.9053\n",
      "Epoch 21/100\n",
      "489899/489899 - 22s - loss: 0.2389 - acc: 0.9071 - val_loss: 0.2427 - val_acc: 0.9058\n",
      "Epoch 22/100\n",
      "489899/489899 - 22s - loss: 0.2384 - acc: 0.9073 - val_loss: 0.2418 - val_acc: 0.9058\n",
      "Epoch 23/100\n",
      "489899/489899 - 22s - loss: 0.2375 - acc: 0.9078 - val_loss: 0.2399 - val_acc: 0.9067\n",
      "Epoch 24/100\n",
      "489899/489899 - 22s - loss: 0.2365 - acc: 0.9081 - val_loss: 0.2363 - val_acc: 0.9085\n",
      "Epoch 25/100\n",
      "489899/489899 - 22s - loss: 0.2356 - acc: 0.9086 - val_loss: 0.2396 - val_acc: 0.9070\n",
      "Epoch 26/100\n",
      "489899/489899 - 22s - loss: 0.2343 - acc: 0.9090 - val_loss: 0.2377 - val_acc: 0.9087\n",
      "Epoch 27/100\n",
      "489899/489899 - 22s - loss: 0.2339 - acc: 0.9095 - val_loss: 0.2413 - val_acc: 0.9063\n",
      "Epoch 28/100\n",
      "489899/489899 - 22s - loss: 0.2339 - acc: 0.9094 - val_loss: 0.2367 - val_acc: 0.9094\n",
      "Epoch 29/100\n",
      "489899/489899 - 22s - loss: 0.2331 - acc: 0.9095 - val_loss: 0.2359 - val_acc: 0.9092\n",
      "Epoch 30/100\n",
      "489899/489899 - 22s - loss: 0.2330 - acc: 0.9096 - val_loss: 0.2379 - val_acc: 0.9085\n",
      "Epoch 31/100\n",
      "489899/489899 - 22s - loss: 0.2311 - acc: 0.9103 - val_loss: 0.2349 - val_acc: 0.9087\n",
      "Epoch 32/100\n"
     ]
    }
   ],
   "source": [
    "history = {\n",
    "    layer: models[layer].fit(pcells_merged_reshaped[layer][pdata_merged.train], \n",
    "                                plabels[pdata_merged.train], \n",
    "                                validation_data=(pcells_merged_reshaped[layer][pdata_merged.val], plabels[pdata_merged.val]),                                   epochs=100, batch_size=200*ngpu, verbose=2)\n",
    "    for layer in mu.cell_meta\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save networks\n",
    "\n",
    "for layer, model in models.items():\n",
    "    model.save(modelpath+\"model_cnn_1x1_\"+layer+\".h5\")\n",
    "    with open(modelpath + \"model_cnn_1x1_\"+layer+\".history\",'wb') as model_history_file:\n",
    "        pickle.dump(history[layer].history, model_history_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
