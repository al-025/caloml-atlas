{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries and some constants\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize, LogNorm\n",
    "import pandas as pd\n",
    "import uproot as ur\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import atlas_mpl_style as ampl\n",
    "ampl.use_atlas_style()\n",
    "\n",
    "path_prefix = '/AL/Phd/maxml/caloml-atlas/'\n",
    "plotpath = path_prefix+'classifier/Plots_var/'\n",
    "modelpath = path_prefix+'classifier/Models/'\n",
    "\n",
    "# import our resolution utilities\n",
    "import sys\n",
    "sys.path.append(path_prefix)\n",
    "from  util import resolution_util as ru\n",
    "from  util import plot_util as pu\n",
    "from  util import ml_util as mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Convolution2D\n",
    "from tensorflow.keras.layers import Convolution3D\n",
    "from tensorflow.keras.layers import MaxPool2D\n",
    "from tensorflow.keras.layers import MaxPool3D\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow import keras as keras\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "gpu_list = [\"/gpu:0\"]\n",
    "strategy = tf.distribute.MirroredStrategy(devices=gpu_list)\n",
    "# strategy = tf.distribute.MirroredStrategy()\n",
    "ngpu = strategy.num_replicas_in_sync\n",
    "print ('Number of devices: {}'.format(ngpu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model_baseline():\n",
    "    with strategy.scope():\n",
    "        input1 = Input(shape=(6, 16, 16), name='input1')\n",
    "        x = Convolution2D(32, (2,2), activation='relu', data_format = 'channels_first')(input1)\n",
    "        x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "        x = Dropout(0.2)(x)\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(128, activation='relu')(x)\n",
    "        x = Dense(64, activation='relu')(x)\n",
    "        output = Dense(2, activation='softmax')(x)\n",
    "        model = Model(inputs = [input1], outputs = [output])\n",
    "        \n",
    "        # compile model\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputpath = path_prefix+'inputs/'\n",
    "dsets = ['nthresh_5','nthresh_10']\n",
    "rootfiles = [\"pi0\", \"piplus\", \"piminus\"]\n",
    "training_classes = ['pi0','piplus']\n",
    "\n",
    "def test_dataset(dataset,size,filename='',\n",
    "                 epochs=100):\n",
    "    if( filename == '' ):\n",
    "        filename = dataset\n",
    "    \n",
    "    trees, pdata = mu.setupPionData(inputpath+dataset+'_', rootfiles)\n",
    "    \n",
    "#     np0 = len(pdata['pi0'])\n",
    "#     npp = len(pdata['piplus'])\n",
    "#     npm = len(pdata['piminus'])\n",
    "\n",
    "#     print(\"Number of pi0 events: {}\".format(np0))\n",
    "#     print(\"Number of pi+ events: {}\".format(npp))\n",
    "#     print(\"Number of pi- events: {}\".format(npm))\n",
    "#     print(\"Total: {}\".format(np0+npp+npm))\n",
    "\n",
    "    pcells = {\n",
    "        ifile : {\n",
    "            layer : mu.setupCells(itree, layer, flatten = False)\n",
    "            for layer in mu.cell_meta\n",
    "        }\n",
    "        for ifile, itree in trees.items()\n",
    "    }\n",
    "    \n",
    "    # create train/validation/test subsets containing 70%/10%/20%\n",
    "    # of events from each type of pion event\n",
    "    pdata_merged, pcells_merged, plabels = mu.createTrainingDatasets(training_classes, pdata, pcells)\n",
    "\n",
    "    pcells_merged_reshaped = mu.reshapeSeparateCNN(pcells_merged)\n",
    "    pcells_EMB2_channels = mu.setupChannelImages(mu.rescaleImages(pcells_merged, (16, 16)))\n",
    "    \n",
    "    # check if model has been trained already\n",
    "    if Path(modelpath+filename+'.h5').is_file():\n",
    "        # load model\n",
    "        model = tf.keras.models.load_model(modelpath+filename+\".h5\")\n",
    "    else:\n",
    "        # train model\n",
    "        f_history = model.fit(pcells_EMB2_channels[pdata_merged.train],\n",
    "                            plabels[pdata_merged.train], \n",
    "                            validation_data=(pcells_EMB2_channels[pdata_merged.val],\n",
    "                                             plabels[pdata_merged.val]),\n",
    "                            epochs=epochs, batch_size=200*ngpu, verbose=2)\n",
    "        f_history = f_history.history\n",
    "\n",
    "        # save trained weights and history\n",
    "        if(filename != ''):\n",
    "            model.save(modelpath+filename+\".h5\")\n",
    "            with open(modelpath+filename +\".history\",'wb') as model_history_file:\n",
    "                pickle.dump(f_history, model_history_file)\n",
    "            \n",
    "    # get network scores for the dataset\n",
    "    f_score = model.predict(\n",
    "        pcells_EMB2_channels\n",
    "    )\n",
    "    \n",
    "    # calculate roc and auc\n",
    "    f_roc_fpr, f_roc_tpr, f_roc_thresh = roc_curve(\n",
    "        plabels[pdata_merged.test][:,1],\n",
    "        f_score[pdata_merged.test,1],\n",
    "        drop_intermediate=False,\n",
    "    )\n",
    "    f_roc_auc = auc(f_roc_fpr, f_roc_tpr)\n",
    "    return f_roc_fpr, f_roc_tpr, f_roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nthresh_fpr = []\n",
    "nthresh_tpr = []\n",
    "nthresh_auc = []\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
